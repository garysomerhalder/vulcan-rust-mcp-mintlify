---
title: Database Integration
description: 'Build production-ready MCP servers with SQLx, connection pooling, and database operations'
---

# Database Integration

Build MCP servers backed by PostgreSQL, MySQL, or SQLite with connection pooling and async operations.

## Overview

<CardGroup cols={3}>
  <Card title="SQLx Integration" icon="database">
    Async database operations with compile-time query verification
  </Card>
  <Card title="Connection Pooling" icon="layer-group">
    Efficient connection management for high throughput
  </Card>
  <Card title="CRUD Tools" icon="wrench">
    Database operations exposed as MCP tools
  </Card>
</CardGroup>

## Complete Example: Task Management Server

A full-featured task management system with PostgreSQL backend.

### Setup

<Tabs>
  <Tab title="Cargo.toml">
    ```toml
    [package]
    name = "task-manager-mcp"
    version = "0.1.0"
    edition = "2021"

    [dependencies]
    vulcan = { version = "0.1", features = ["server", "transport-io"] }
    tokio = { version = "1", features = ["full"] }
    sqlx = { version = "0.8", features = ["runtime-tokio", "tls-rustls", "postgres", "migrate", "chrono", "uuid"] }
    serde = { version = "1", features = ["derive"] }
    serde_json = "1"
    chrono = { version = "0.4", features = ["serde"] }
    uuid = { version = "1", features = ["v4", "serde"] }
    tracing = "0.1"
    tracing-subscriber = "0.3"
    anyhow = "1"
    ```
  </Tab>

  <Tab title="Database Schema">
    ```sql
    -- migrations/001_create_tasks.sql
    CREATE TABLE IF NOT EXISTS tasks (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        title VARCHAR(255) NOT NULL,
        description TEXT,
        status VARCHAR(50) NOT NULL DEFAULT 'pending',
        priority INTEGER NOT NULL DEFAULT 0,
        created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
        completed_at TIMESTAMP WITH TIME ZONE
    );

    CREATE INDEX idx_tasks_status ON tasks(status);
    CREATE INDEX idx_tasks_priority ON tasks(priority DESC);
    CREATE INDEX idx_tasks_created ON tasks(created_at DESC);

    -- Trigger to auto-update updated_at
    CREATE OR REPLACE FUNCTION update_updated_at_column()
    RETURNS TRIGGER AS $$
    BEGIN
        NEW.updated_at = NOW();
        RETURN NEW;
    END;
    $$ language 'plpgsql';

    CREATE TRIGGER update_tasks_updated_at BEFORE UPDATE ON tasks
        FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
    ```
  </Tab>

  <Tab title="Environment">
    ```bash
    # .env
    DATABASE_URL=postgresql://user:password@localhost:5432/taskdb
    RUST_LOG=info
    ```
  </Tab>
</Tabs>

### Implementation

<Tabs>
  <Tab title="Server Implementation">
    ```rust
    use vulcan::{
        prelude::*,
        tool, tool_router, tool_handler, resource, resource_router, resource_handler,
        handler::server::{tool::ToolRouter, resource::ResourceRouter},
    };
    use sqlx::{PgPool, FromRow, postgres::PgPoolOptions};
    use serde::{Serialize, Deserialize};
    use uuid::Uuid;
    use chrono::{DateTime, Utc};
    use std::time::Duration;

    // Domain model
    #[derive(Debug, Clone, Serialize, Deserialize, FromRow)]
    pub struct Task {
        pub id: Uuid,
        pub title: String,
        pub description: Option<String>,
        pub status: String,
        pub priority: i32,
        pub created_at: DateTime<Utc>,
        pub updated_at: DateTime<Utc>,
        pub completed_at: Option<DateTime<Utc>>,
    }

    #[derive(Debug, Serialize, Deserialize)]
    pub struct CreateTaskInput {
        pub title: String,
        pub description: Option<String>,
        pub priority: Option<i32>,
    }

    #[derive(Debug, Serialize, Deserialize)]
    pub struct UpdateTaskInput {
        pub title: Option<String>,
        pub description: Option<String>,
        pub status: Option<String>,
        pub priority: Option<i32>,
    }

    #[derive(Debug, Serialize, Deserialize)]
    pub struct TaskFilters {
        pub status: Option<String>,
        pub min_priority: Option<i32>,
        pub limit: Option<i64>,
    }

    // MCP Server
    #[derive(Clone)]
    pub struct TaskServer {
        pool: PgPool,
        tool_router: ToolRouter<Self>,
        resource_router: ResourceRouter<Self>,
    }

    #[tool_router]
    #[resource_router]
    impl TaskServer {
        pub async fn new(database_url: &str) -> Result<Self, ErrorData> {
            // Create connection pool with tuning
            let pool = PgPoolOptions::new()
                .max_connections(20)
                .min_connections(5)
                .acquire_timeout(Duration::from_secs(5))
                .idle_timeout(Duration::from_secs(600))
                .max_lifetime(Duration::from_secs(1800))
                .connect(database_url)
                .await
                .map_err(|e| ErrorData::internal(format!("Failed to connect to database: {}", e)))?;

            // Run migrations
            sqlx::migrate!("./migrations")
                .run(&pool)
                .await
                .map_err(|e| ErrorData::internal(format!("Failed to run migrations: {}", e)))?;

            Ok(Self {
                pool,
                tool_router: Self::tool_router(),
                resource_router: Self::resource_router(),
            })
        }

        // ==================== TOOLS (Write Operations) ====================

        #[tool(description = "Create a new task")]
        async fn create_task(
            &self,
            title: String,
            description: Option<String>,
            priority: Option<i32>,
        ) -> Result<Task, ErrorData> {
            let priority = priority.unwrap_or(0);

            let task = sqlx::query_as::<_, Task>(
                r#"
                INSERT INTO tasks (title, description, priority, status)
                VALUES ($1, $2, $3, 'pending')
                RETURNING *
                "#
            )
            .bind(&title)
            .bind(&description)
            .bind(priority)
            .fetch_one(&self.pool)
            .await
            .map_err(|e| ErrorData::internal(format!("Failed to create task: {}", e)))?;

            tracing::info!("Created task: {} ({})", task.title, task.id);
            Ok(task)
        }

        #[tool(description = "Update an existing task")]
        async fn update_task(
            &self,
            id: String,
            title: Option<String>,
            description: Option<String>,
            status: Option<String>,
            priority: Option<i32>,
        ) -> Result<Task, ErrorData> {
            let task_id = Uuid::parse_str(&id)
                .map_err(|_| ErrorData::invalid_input("Invalid UUID format"))?;

            // Validate status if provided
            if let Some(ref s) = status {
                match s.as_str() {
                    "pending" | "in_progress" | "completed" | "cancelled" => {},
                    _ => return Err(ErrorData::invalid_input(
                        "Status must be: pending, in_progress, completed, or cancelled"
                    )),
                }
            }

            // Build dynamic update query
            let mut query = String::from("UPDATE tasks SET updated_at = NOW()");
            let mut bind_count = 1;

            if title.is_some() {
                query.push_str(&format!(", title = ${}", bind_count));
                bind_count += 1;
            }
            if description.is_some() {
                query.push_str(&format!(", description = ${}", bind_count));
                bind_count += 1;
            }
            if status.is_some() {
                query.push_str(&format!(", status = ${}", bind_count));
                // Set completed_at if status is 'completed'
                if status.as_ref().unwrap() == "completed" {
                    query.push_str(", completed_at = NOW()");
                }
                bind_count += 1;
            }
            if priority.is_some() {
                query.push_str(&format!(", priority = ${}", bind_count));
                bind_count += 1;
            }

            query.push_str(&format!(" WHERE id = ${} RETURNING *", bind_count));

            // Execute query with bindings
            let mut q = sqlx::query_as::<_, Task>(&query);

            if let Some(t) = title { q = q.bind(t); }
            if let Some(d) = description { q = q.bind(d); }
            if let Some(s) = status { q = q.bind(s); }
            if let Some(p) = priority { q = q.bind(p); }
            q = q.bind(task_id);

            let task = q.fetch_one(&self.pool)
                .await
                .map_err(|e| match e {
                    sqlx::Error::RowNotFound => ErrorData::not_found(format!("Task {} not found", id)),
                    _ => ErrorData::internal(format!("Failed to update task: {}", e)),
                })?;

            tracing::info!("Updated task: {} ({})", task.title, task.id);
            Ok(task)
        }

        #[tool(description = "Delete a task")]
        async fn delete_task(&self, id: String) -> Result<String, ErrorData> {
            let task_id = Uuid::parse_str(&id)
                .map_err(|_| ErrorData::invalid_input("Invalid UUID format"))?;

            let result = sqlx::query("DELETE FROM tasks WHERE id = $1")
                .bind(task_id)
                .execute(&self.pool)
                .await
                .map_err(|e| ErrorData::internal(format!("Failed to delete task: {}", e)))?;

            if result.rows_affected() == 0 {
                return Err(ErrorData::not_found(format!("Task {} not found", id)));
            }

            tracing::info!("Deleted task: {}", task_id);
            Ok(format!("Task {} deleted successfully", id))
        }

        #[tool(description = "Mark task as completed")]
        async fn complete_task(&self, id: String) -> Result<Task, ErrorData> {
            self.update_task(id, None, None, Some("completed".to_string()), None).await
        }

        #[tool(description = "List tasks with optional filters")]
        async fn list_tasks(
            &self,
            status: Option<String>,
            min_priority: Option<i32>,
            limit: Option<i64>,
        ) -> Result<Vec<Task>, ErrorData> {
            let limit = limit.unwrap_or(100).min(1000); // Cap at 1000

            let mut query = String::from("SELECT * FROM tasks WHERE 1=1");
            let mut bind_count = 0;

            if status.is_some() {
                bind_count += 1;
                query.push_str(&format!(" AND status = ${}", bind_count));
            }

            if min_priority.is_some() {
                bind_count += 1;
                query.push_str(&format!(" AND priority >= ${}", bind_count));
            }

            query.push_str(" ORDER BY priority DESC, created_at DESC");
            bind_count += 1;
            query.push_str(&format!(" LIMIT ${}", bind_count));

            let mut q = sqlx::query_as::<_, Task>(&query);

            if let Some(s) = status { q = q.bind(s); }
            if let Some(p) = min_priority { q = q.bind(p); }
            q = q.bind(limit);

            let tasks = q.fetch_all(&self.pool)
                .await
                .map_err(|e| ErrorData::internal(format!("Failed to list tasks: {}", e)))?;

            Ok(tasks)
        }

        #[tool(description = "Get task statistics")]
        async fn get_stats(&self) -> Result<serde_json::Value, ErrorData> {
            let stats = sqlx::query!(
                r#"
                SELECT
                    COUNT(*) as total,
                    COUNT(*) FILTER (WHERE status = 'pending') as pending,
                    COUNT(*) FILTER (WHERE status = 'in_progress') as in_progress,
                    COUNT(*) FILTER (WHERE status = 'completed') as completed,
                    COUNT(*) FILTER (WHERE status = 'cancelled') as cancelled,
                    AVG(priority) as avg_priority
                FROM tasks
                "#
            )
            .fetch_one(&self.pool)
            .await
            .map_err(|e| ErrorData::internal(format!("Failed to get stats: {}", e)))?;

            Ok(serde_json::json!({
                "total": stats.total.unwrap_or(0),
                "pending": stats.pending.unwrap_or(0),
                "in_progress": stats.in_progress.unwrap_or(0),
                "completed": stats.completed.unwrap_or(0),
                "cancelled": stats.cancelled.unwrap_or(0),
                "average_priority": stats.avg_priority.unwrap_or(0.0),
            }))
        }

        // ==================== RESOURCES (Read-Only Access) ====================

        #[resource(
            uri = "task:///{id}",
            name = "Task",
            description = "Get task by ID",
            mime_type = "application/json"
        )]
        async fn get_task(&self, id: String) -> Result<ResourceContents, ErrorData> {
            let task_id = Uuid::parse_str(&id)
                .map_err(|_| ErrorData::invalid_input("Invalid UUID format"))?;

            let task = sqlx::query_as::<_, Task>("SELECT * FROM tasks WHERE id = $1")
                .bind(task_id)
                .fetch_one(&self.pool)
                .await
                .map_err(|e| match e {
                    sqlx::Error::RowNotFound => ErrorData::not_found(format!("Task {} not found", id)),
                    _ => ErrorData::internal(format!("Failed to fetch task: {}", e)),
                })?;

            let json = serde_json::to_string_pretty(&task)
                .map_err(|e| ErrorData::internal(format!("Serialization error: {}", e)))?;

            Ok(ResourceContents::text(json))
        }

        #[resource(
            uri = "task:///all",
            name = "All Tasks",
            description = "Get all tasks",
            mime_type = "application/json"
        )]
        async fn get_all_tasks(&self) -> Result<ResourceContents, ErrorData> {
            let tasks = sqlx::query_as::<_, Task>(
                "SELECT * FROM tasks ORDER BY priority DESC, created_at DESC LIMIT 100"
            )
            .fetch_all(&self.pool)
            .await
            .map_err(|e| ErrorData::internal(format!("Failed to fetch tasks: {}", e)))?;

            let json = serde_json::to_string_pretty(&tasks)
                .map_err(|e| ErrorData::internal(format!("Serialization error: {}", e)))?;

            Ok(ResourceContents::text(json))
        }

        #[resource(
            uri = "task:///stats",
            name = "Task Statistics",
            description = "Get task statistics",
            mime_type = "application/json"
        )]
        async fn get_stats_resource(&self) -> Result<ResourceContents, ErrorData> {
            let stats = self.get_stats().await?;
            let json = serde_json::to_string_pretty(&stats)
                .map_err(|e| ErrorData::internal(format!("Serialization error: {}", e)))?;

            Ok(ResourceContents::text(json))
        }
    }

    // ==================== SERVER HANDLER ====================

    #[tool_handler]
    #[resource_handler]
    impl ServerHandler for TaskServer {
        fn get_info(&self) -> ServerInfo {
            ServerInfo {
                protocol_version: ProtocolVersion::V_2024_11_05,
                capabilities: ServerCapabilities::builder()
                    .enable_tools()
                    .enable_resources()
                    .build(),
                server_info: Implementation {
                    name: "task-manager".to_string(),
                    version: "1.0.0".to_string(),
                    ..Default::default()
                },
                instructions: Some(
                    "Task management system with database backend. \
                     Use tools for CRUD operations, resources for read-only access.".to_string()
                ),
            }
        }

        async fn list_resources(&self) -> Result<Vec<Resource>, ErrorData> {
            // Dynamic resource list based on database content
            let tasks = sqlx::query_as::<_, Task>("SELECT * FROM tasks LIMIT 100")
                .fetch_all(&self.pool)
                .await
                .map_err(|e| ErrorData::internal(format!("Failed to fetch tasks: {}", e)))?;

            let mut resources = vec![
                Resource {
                    uri: "task:///all".to_string(),
                    name: "All Tasks".to_string(),
                    description: Some("List of all tasks".to_string()),
                    mime_type: Some("application/json".to_string()),
                },
                Resource {
                    uri: "task:///stats".to_string(),
                    name: "Statistics".to_string(),
                    description: Some("Task statistics".to_string()),
                    mime_type: Some("application/json".to_string()),
                },
            ];

            for task in tasks {
                resources.push(Resource {
                    uri: format!("task:///{}", task.id),
                    name: task.title.clone(),
                    description: Some(format!("Task: {} ({})", task.title, task.status)),
                    mime_type: Some("application/json".to_string()),
                });
            }

            Ok(resources)
        }
    }

    // ==================== MAIN ====================

    #[tokio::main]
    async fn main() -> Result<(), Box<dyn std::error::Error>> {
        // Initialize tracing
        tracing_subscriber::fmt::init();

        // Load environment variables
        dotenv::dotenv().ok();
        let database_url = std::env::var("DATABASE_URL")
            .expect("DATABASE_URL must be set");

        // Create server
        let server = TaskServer::new(&database_url).await?;
        tracing::info!("Task manager server initialized");

        // Serve via stdio for Claude Desktop
        let transport = StdioTransport::new();
        server.serve(transport).await?;

        Ok(())
    }
    ```
  </Tab>

  <Tab title="Testing">
    ```rust
    #[cfg(test)]
    mod tests {
        use super::*;
        use vulcan::transport::duplex;

        async fn test_server() -> TaskServer {
            // Use in-memory SQLite for testing
            TaskServer::new("sqlite::memory:").await.unwrap()
        }

        #[tokio::test]
        async fn test_create_task() {
            let server = test_server().await;

            let task = server.create_task(
                "Test Task".to_string(),
                Some("Description".to_string()),
                Some(5),
            ).await.unwrap();

            assert_eq!(task.title, "Test Task");
            assert_eq!(task.priority, 5);
            assert_eq!(task.status, "pending");
        }

        #[tokio::test]
        async fn test_update_task() {
            let server = test_server().await;

            let task = server.create_task("Test".to_string(), None, None).await.unwrap();
            let updated = server.update_task(
                task.id.to_string(),
                Some("Updated".to_string()),
                None,
                Some("completed".to_string()),
                None,
            ).await.unwrap();

            assert_eq!(updated.title, "Updated");
            assert_eq!(updated.status, "completed");
            assert!(updated.completed_at.is_some());
        }

        #[tokio::test]
        async fn test_list_tasks() {
            let server = test_server().await;

            server.create_task("Task 1".to_string(), None, Some(1)).await.unwrap();
            server.create_task("Task 2".to_string(), None, Some(5)).await.unwrap();

            let tasks = server.list_tasks(None, Some(2), None).await.unwrap();
            assert_eq!(tasks.len(), 1); // Only Task 2 has priority >= 2
            assert_eq!(tasks[0].title, "Task 2");
        }

        #[tokio::test]
        async fn test_complete_task() {
            let server = test_server().await;

            let task = server.create_task("Complete Me".to_string(), None, None).await.unwrap();
            let completed = server.complete_task(task.id.to_string()).await.unwrap();

            assert_eq!(completed.status, "completed");
            assert!(completed.completed_at.is_some());
        }

        #[tokio::test]
        async fn test_stats() {
            let server = test_server().await;

            server.create_task("Task 1".to_string(), None, None).await.unwrap();
            let task2 = server.create_task("Task 2".to_string(), None, None).await.unwrap();
            server.complete_task(task2.id.to_string()).await.unwrap();

            let stats = server.get_stats().await.unwrap();
            assert_eq!(stats["total"], 2);
            assert_eq!(stats["pending"], 1);
            assert_eq!(stats["completed"], 1);
        }

        #[tokio::test]
        async fn test_resource_access() {
            let server = test_server().await;
            let task = server.create_task("Resource Test".to_string(), None, None).await.unwrap();

            let resource = server.get_task(task.id.to_string()).await.unwrap();
            let json: Task = serde_json::from_str(resource.text().unwrap()).unwrap();

            assert_eq!(json.id, task.id);
            assert_eq!(json.title, "Resource Test");
        }

        #[tokio::test]
        async fn test_integration_with_client() {
            let (client_transport, server_transport) = duplex();

            let server = test_server().await;
            tokio::spawn(async move {
                server.serve(server_transport).await.ok();
            });

            let client = ().serve(client_transport).await.unwrap();

            // Create task via MCP
            let result = client.call_tool(
                "create_task",
                serde_json::json!({
                    "title": "Integration Test",
                    "priority": 10
                }),
                None,
            ).await.unwrap();

            assert!(result.is_success());
        }
    }
    ```
  </Tab>

  <Tab title="Docker Deployment">
    ```dockerfile
    FROM rust:1.75 as builder

    WORKDIR /app
    COPY . .

    RUN cargo build --release

    FROM debian:bookworm-slim

    RUN apt-get update && apt-get install -y \
        ca-certificates \
        libpq5 \
        && rm -rf /var/lib/apt/lists/*

    COPY --from=builder /app/target/release/task-manager-mcp /usr/local/bin/
    COPY --from=builder /app/migrations /migrations

    ENV DATABASE_URL=postgresql://user:password@db:5432/taskdb
    ENV RUST_LOG=info

    CMD ["task-manager-mcp"]
    ```

    ```yaml
    # docker-compose.yml
    version: '3.8'

    services:
      db:
        image: postgres:16
        environment:
          POSTGRES_USER: user
          POSTGRES_PASSWORD: password
          POSTGRES_DB: taskdb
        volumes:
          - postgres_data:/var/lib/postgresql/data
        ports:
          - "5432:5432"

      mcp-server:
        build: .
        depends_on:
          - db
        environment:
          DATABASE_URL: postgresql://user:password@db:5432/taskdb
          RUST_LOG: info
        restart: unless-stopped

    volumes:
      postgres_data:
    ```
  </Tab>
</Tabs>

## Best Practices

### 1. Connection Pooling

<AccordionGroup>
  <Accordion title="Pool Configuration">
    ```rust
    let pool = PgPoolOptions::new()
        .max_connections(20)           // Max concurrent connections
        .min_connections(5)            // Keep alive for fast response
        .acquire_timeout(Duration::from_secs(5))  // Fail fast on pool exhaustion
        .idle_timeout(Duration::from_secs(600))   // Close idle connections after 10 min
        .max_lifetime(Duration::from_secs(1800))  // Rotate connections after 30 min
        .test_before_acquire(true)     // Verify connection health
        .connect(database_url)
        .await?;
    ```

    **Sizing guidelines:**
    - Start with `max_connections = 2 * CPU cores`
    - Monitor with `pool.size()` and `pool.num_idle()`
    - Adjust based on query latency and throughput
  </Accordion>

  <Accordion title="Connection Health">
    ```rust
    // Periodic health check
    async fn health_check(pool: &PgPool) -> Result<(), ErrorData> {
        sqlx::query("SELECT 1")
            .fetch_one(pool)
            .await
            .map_err(|e| ErrorData::internal(format!("Health check failed: {}", e)))?;
        Ok(())
    }

    // Use in tool:
    #[tool(description = "Check database health")]
    async fn health(&self) -> Result<String, ErrorData> {
        health_check(&self.pool).await?;
        Ok("Database is healthy".to_string())
    }
    ```
  </Accordion>
</AccordionGroup>

### 2. Error Handling

<AccordionGroup>
  <Accordion title="Classify Database Errors">
    ```rust
    fn map_sqlx_error(e: sqlx::Error) -> ErrorData {
        match e {
            sqlx::Error::RowNotFound => {
                ErrorData::not_found("Resource not found")
            }
            sqlx::Error::Database(db_err) => {
                // PostgreSQL error codes: https://www.postgresql.org/docs/current/errcodes-appendix.html
                if let Some(code) = db_err.code() {
                    match code.as_ref() {
                        "23505" => ErrorData::invalid_input("Duplicate key violation"),
                        "23503" => ErrorData::invalid_input("Foreign key violation"),
                        "23502" => ErrorData::invalid_input("Not null violation"),
                        _ => ErrorData::internal(format!("Database error: {}", db_err)),
                    }
                } else {
                    ErrorData::internal(format!("Database error: {}", db_err))
                }
            }
            sqlx::Error::PoolTimedOut => {
                ErrorData::internal("Connection pool exhausted - try again later")
            }
            _ => ErrorData::internal(format!("Database error: {}", e)),
        }
    }

    // Usage:
    let task = sqlx::query_as::<_, Task>("SELECT * FROM tasks WHERE id = $1")
        .bind(id)
        .fetch_one(&self.pool)
        .await
        .map_err(map_sqlx_error)?;
    ```
  </Accordion>

  <Accordion title="Transaction Rollback">
    ```rust
    #[tool(description = "Transfer task ownership (atomic)")]
    async fn transfer_ownership(
        &self,
        task_id: String,
        from_user: String,
        to_user: String,
    ) -> Result<String, ErrorData> {
        let task_id = Uuid::parse_str(&task_id)?;

        let mut tx = self.pool.begin().await
            .map_err(|e| ErrorData::internal(format!("Failed to start transaction: {}", e)))?;

        // Verify ownership
        let current_owner = sqlx::query_scalar::<_, String>(
            "SELECT owner FROM tasks WHERE id = $1"
        )
        .bind(task_id)
        .fetch_one(&mut *tx)
        .await
        .map_err(map_sqlx_error)?;

        if current_owner != from_user {
            tx.rollback().await.ok();
            return Err(ErrorData::invalid_input("Task not owned by from_user"));
        }

        // Transfer ownership
        sqlx::query("UPDATE tasks SET owner = $1, updated_at = NOW() WHERE id = $2")
            .bind(&to_user)
            .bind(task_id)
            .execute(&mut *tx)
            .await
            .map_err(map_sqlx_error)?;

        // Log transfer
        sqlx::query(
            "INSERT INTO task_history (task_id, action, from_user, to_user) VALUES ($1, 'transfer', $2, $3)"
        )
        .bind(task_id)
        .bind(&from_user)
        .bind(&to_user)
        .execute(&mut *tx)
        .await
        .map_err(map_sqlx_error)?;

        tx.commit().await
            .map_err(|e| ErrorData::internal(format!("Failed to commit: {}", e)))?;

        Ok(format!("Transferred task {} from {} to {}", task_id, from_user, to_user))
    }
    ```
  </Accordion>
</AccordionGroup>

### 3. Query Optimization

<Tip>
Use `EXPLAIN ANALYZE` to profile queries and add indexes for frequently filtered columns.
</Tip>

```rust
// ✅ Good - Indexed query
sqlx::query_as::<_, Task>(
    "SELECT * FROM tasks WHERE status = $1 ORDER BY priority DESC LIMIT $2"
)
.bind("pending")
.bind(10)
.fetch_all(&pool)
.await?;

// ❌ Bad - Full table scan
sqlx::query_as::<_, Task>(
    "SELECT * FROM tasks WHERE LOWER(title) LIKE $1"  // No index on LOWER(title)
)
.bind("%search%")
.fetch_all(&pool)
.await?;

// ✅ Better - Use full-text search with GIN index
sqlx::query_as::<_, Task>(
    "SELECT * FROM tasks WHERE to_tsvector('english', title) @@ plainto_tsquery('english', $1)"
)
.bind("search")
.fetch_all(&pool)
.await?;
```

### 4. Migrations

<AccordionGroup>
  <Accordion title="Running Migrations">
    ```rust
    // Embed migrations in binary
    sqlx::migrate!("./migrations")
        .run(&pool)
        .await
        .map_err(|e| ErrorData::internal(format!("Migrations failed: {}", e)))?;
    ```

    ```bash
    # Create new migration
    sqlx migrate add create_users_table

    # Apply migrations manually
    sqlx migrate run --database-url postgresql://...

    # Revert last migration
    sqlx migrate revert --database-url postgresql://...
    ```
  </Accordion>

  <Accordion title="Migration Best Practices">
    - **Idempotent:** Use `IF NOT EXISTS`, `IF EXISTS`
    - **Reversible:** Always provide `DOWN` migration
    - **Small:** One logical change per migration
    - **Tested:** Test migrations on staging before production
    - **Backward-compatible:** Don't break running servers

    ```sql
    -- ✅ Good - Idempotent
    CREATE TABLE IF NOT EXISTS users (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid()
    );

    -- ✅ Good - Safe column add
    ALTER TABLE tasks ADD COLUMN IF NOT EXISTS owner_id UUID;

    -- ❌ Bad - Not idempotent
    CREATE TABLE users (id UUID PRIMARY KEY);

    -- ❌ Bad - Breaking change
    ALTER TABLE tasks DROP COLUMN status;  -- Breaks running servers!
    ```
  </Accordion>
</AccordionGroup>

## Performance Tuning

### Monitoring

```rust
use std::sync::atomic::{AtomicU64, Ordering};

#[derive(Clone)]
pub struct TaskServer {
    pool: PgPool,
    query_count: Arc<AtomicU64>,
    tool_router: ToolRouter<Self>,
}

impl TaskServer {
    #[tool(description = "Get performance metrics")]
    async fn metrics(&self) -> Result<serde_json::Value, ErrorData> {
        Ok(serde_json::json!({
            "pool_size": self.pool.size(),
            "pool_idle": self.pool.num_idle(),
            "total_queries": self.query_count.load(Ordering::Relaxed),
        }))
    }

    // Increment counter in every query
    async fn track_query<T>(&self, f: impl Future<Output = Result<T, sqlx::Error>>) -> Result<T, sqlx::Error> {
        self.query_count.fetch_add(1, Ordering::Relaxed);
        f.await
    }
}
```

### Caching

```rust
use moka::future::Cache;
use std::time::Duration;

#[derive(Clone)]
pub struct CachedTaskServer {
    pool: PgPool,
    cache: Cache<Uuid, Task>,
}

impl CachedTaskServer {
    pub fn new(pool: PgPool) -> Self {
        let cache = Cache::builder()
            .max_capacity(1000)
            .time_to_live(Duration::from_secs(300))  // 5 min TTL
            .build();

        Self { pool, cache }
    }

    async fn get_task_cached(&self, id: Uuid) -> Result<Task, ErrorData> {
        if let Some(task) = self.cache.get(&id).await {
            return Ok(task);
        }

        let task = sqlx::query_as::<_, Task>("SELECT * FROM tasks WHERE id = $1")
            .bind(id)
            .fetch_one(&self.pool)
            .await
            .map_err(map_sqlx_error)?;

        self.cache.insert(id, task.clone()).await;
        Ok(task)
    }

    // Invalidate cache on update
    async fn update_task_cached(&self, id: Uuid, /* ... */) -> Result<Task, ErrorData> {
        let task = self.update_task_impl(id, /* ... */).await?;
        self.cache.invalidate(&id).await;
        Ok(task)
    }
}
```

## Next Steps

<CardGroup cols={2}>
  <Card title="Working with Resources" icon="database" href="/guides/working-with-resources">
    Expose database data as MCP resources
  </Card>
  <Card title="Advanced Patterns" icon="brain" href="/guides/advanced-patterns">
    Rate limiting, caching, graceful shutdown
  </Card>
  <Card title="Testing Guide" icon="vial" href="/guides/testing">
    Test database operations
  </Card>
  <Card title="Deployment" icon="rocket" href="/guides/deployment">
    Deploy to production with Postgres
  </Card>
</CardGroup>
